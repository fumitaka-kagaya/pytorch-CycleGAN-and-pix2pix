{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOi03Sm76SDt64WeM0Pk0aM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fumitaka-kagaya/pytorch-CycleGAN-and-pix2pix/blob/master/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install --quiet gradio==4.44.0 torch torchaudio librosa matplotlib numpy soundfile\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "import torchaudio\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import io\n",
        "\n",
        "# ===== 1) 定義：ラベルとモデルクラス =====\n",
        "LABELS = [\"corrosion\", \"crack\", \"healthy\", \"hinge\"]\n",
        "\n",
        "# SimpleAudioCNN（以前作ったモデル構造をここに記述）\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleAudioCNN(nn.Module):\n",
        "def __init__(self, num_classes=4):\n",
        "super(SimpleAudioCNN, self).__init__()\n",
        "self.conv1 = nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1)\n",
        "self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
        "self.pool = nn.MaxPool2d(2, 2)\n",
        "self.fc1 = nn.Linear(16*16*16, 128) # データサイズに合わせて変更\n",
        "self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "def forward(self, x):\n",
        "x = self.pool(F.relu(self.conv1(x)))\n",
        "x = self.pool(F.relu(self.conv2(x)))\n",
        "x = x.view(x.size(0), -1)\n",
        "x = F.relu(self.fc1(x))\n",
        "x = self.fc2(x)\n",
        "return x\n",
        "\n",
        "# ===== 2) 推論関数 =====\n",
        "def predict_audio(audio_file, model_file):\n",
        "if audio_file is None:\n",
        "return \"音声ファイルをアップロードしてください\", None\n",
        "if model_file is None:\n",
        "return \"モデルファイル（.pth）をアップロードしてください\", None\n",
        "\n",
        "# 2-1) モデルロード\n",
        "try:\n",
        "model = SimpleAudioCNN(num_classes=len(LABELS))\n",
        "state_dict = torch.load(model_file.name, map_location=\"cpu\")\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "except Exception as e:\n",
        "return f\"モデルロード失敗: {e}\", None\n",
        "\n",
        "# 2-2) 音声読み込み\n",
        "try:\n",
        "waveform, sr = torchaudio.load(audio_file.name)\n",
        "if waveform.shape[0] > 1:\n",
        "waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "transform = torchaudio.transforms.MelSpectrogram(sample_rate=sr, n_mels=64)\n",
        "mel_spec = transform(waveform)\n",
        "mel_spec = mel_spec.unsqueeze(0) # (1, 1, freq, time)\n",
        "except Exception as e:\n",
        "return f\"音声読み込み失敗: {e}\", None\n",
        "\n",
        "# 2-3) 推論\n",
        "with torch.no_grad():\n",
        "output = model(mel_spec)\n",
        "pred_idx = torch.argmax(output, dim=1).item()\n",
        "pred_label = LABELS[pred_idx]\n",
        "\n",
        "# 2-4) スペクトログラム描画\n",
        "mel_db = 10 * torch.log10(mel_spec[0][0] + 1e-6)\n",
        "fig, ax = plt.subplots(figsize=(6,3))\n",
        "img = librosa.display.specshow(mel_db.numpy(), sr=sr, x_axis='time', y_axis='mel', ax=ax)\n",
        "ax.set(title=\"Mel spectrogram\")\n",
        "fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n",
        "buf = io.BytesIO()\n",
        "plt.tight_layout()\n",
        "plt.savefig(buf, format=\"png\")\n",
        "plt.close(fig)\n",
        "buf.seek(0)\n",
        "\n",
        "return f\"推論結果: {pred_label}\", buf\n",
        "\n",
        "# ===== 3) Gradio GUI =====\n",
        "iface = gr.Interface(\n",
        "fn=predict_audio,\n",
        "inputs=[\n",
        "gr.Audio(type=\"filepath\", label=\"音声ファイルをアップロード (.wav)\"),\n",
        "gr.File(file_types=[\".pth\"], label=\"学習済みモデルファイル (.pth)\")\n",
        "],\n",
        "outputs=[gr.Textbox(label=\"推論結果\"), gr.Image(label=\"スペクトログラム\")],\n",
        "title=\"マンホール打音分類（PyTorchモデル推論）\",\n",
        "description=\"アップロードした音声を学習済み PyTorch モデルで分類します。外部接続なし、Colab 内安全実行。\",\n",
        "allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "iface.launch(share=False)"
      ],
      "metadata": {
        "id": "UdCA9waVyWUQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}